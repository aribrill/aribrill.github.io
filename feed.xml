<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aribrill.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aribrill.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-08T18:07:48+00:00</updated><id>https://aribrill.com/feed.xml</id><title type="html">blank</title><subtitle>AI safety researcher and PhD astrophysicist </subtitle><entry><title type="html">Computational Complexity as an Intuition Pump for LLM Generality</title><link href="https://aribrill.com/blog/2024/computational-complexity/" rel="alternate" type="text/html" title="Computational Complexity as an Intuition Pump for LLM Generality"/><published>2024-06-25T23:59:59+00:00</published><updated>2024-06-25T23:59:59+00:00</updated><id>https://aribrill.com/blog/2024/computational-complexity</id><content type="html" xml:base="https://aribrill.com/blog/2024/computational-complexity/"><![CDATA[<p><em>This post was originally published on <a href="https://www.lesswrong.com/posts/HxHYPJoaWD8AjaHCk/computational-complexity-as-an-intuition-pump-for-llm">LessWrong</a>.</em></p> <p>With sufficient scale and scaffolding, LLMs will improve without bound on all tasks to become superhuman AGI, to the extent they haven’t already. No, wait! LLMs are dead-end pattern-matching machines fundamentally incapable of general reasoning and novel problem solving. Which is it?</p> <p>I’ll call these opposing points of view “LLM Scaler” and “LLM Skeptic”. The first appears to be held by the big AGI labs and was recently exemplified by Leopold Aschenbrenner’s <a href="https://situational-awareness.ai/">Situational Awareness</a> series, while the second is influenced by cognitive science and can be associated with researchers such as François Chollet, Gary Marcus, and Melanie Mitchell. This post loosely generalizes these two stances, so any misrepresentation or conflation of individuals’ viewpoints is my own. I recommend Egg Syntax’s <a href="https://www.lesswrong.com/posts/k38sJNLk7YbJA72ST/llm-generality-is-a-timeline-crux">recent post</a> for further introduction and a summary of relevant research. We can caricature the debate something like this:</p> <p>LLM Scaler: LLMs will take us to AGI. See straight lines on graphs. See correct answers to hard questions on arbitrary topics. See real customers paying real money for real value.</p> <p>LLM Skeptic: LLMs achieve high skill by memorizing patterns inherent in their giant training set. This is not the path to general intelligence. For example, LLMs will never solve Task X.</p> <p>LLM Scaler: They solved it.</p> <p>LLM Skeptic: No kidding? That doesn’t count then, but they’ll really never solve Task Y. And you can’t just keep scaling. That will cost trillions of dollars.</p> <p>LLM Scaler: Do you want to see my pitch deck? <br/> <br/> <br/> Meanwhile, on Alternate Earth, Silicon Valley is abuzz over recent progress by Large List Manipulators (LLMs), which sort a list by iteratively <a href="https://en.wikipedia.org/wiki/Insertion_sort">inserting</a> each item into its correct location. Startups scramble to secure special-purpose hardware for speeding up their LLMs.</p> <p>LLM Scaler: LLMs are general list sorters, and will scale to sort lists of any size. Sure, we don’t quite understand how they work, but our empirical compute-optimal scaling law (N ~ C^0.5) has already held across a dozen OOMs, and we’re spending billions in venture capital to keep it going!</p> <p>LLM Skeptic: That absurd expense is unsustainable. Can’t you see that? There’s no way that LLMs are truly general list sorters. Good luck getting one to sort a list with a million items.</p> <p>LLM Scaler: We already have.</p> <p>LLM Skeptic: Oh. Well then, LLMs will never sort a list with a BILLION items!</p> <p>The “LLM” Skeptic is, literally, wrong. Insertion sort is fully general and can in principle scale to sort a list of any size. Each time the Skeptic declares that some specific list size is impossible to sort, they only embarrass themselves. But this skepticism reflects a deeper truth. The “LLM” paradigm is fundamentally inefficient. Sooner or later, hype will crash into real-world cost constraints and progress will stall. If Alternate Earth knew more computer science, the Skeptic would have told the Scaler to replace O(N^2) insertion sort with an efficient O(N log N) algorithm like <a href="https://en.wikipedia.org/wiki/Quicksort">quicksort</a>. <br/> <br/> <br/> Returning to Large Language Models, how might computational complexity reframe our understanding of their abilities? The explicit or implicit designer of any intelligent system faces a tradeoff between allocating its model capacity into memorization – of facts, heuristics, or complex programmed behaviors – and implementing adaptive algorithms for learning or <a href="https://arxiv.org/abs/1906.01820">optimization</a>. Both strategies enable equivalent behavior given unlimited resources. However, the former strategy requires model capacity that scales with the diversity and complexity of possible tasks, and the real world is both diverse and complex. The latter strategy requires only constant capacity, yielding drastically improved efficiency. This is made possible by exploiting some combination of in-context data, computation time, or external memory during inference.</p> <p>An efficient learning algorithm makes the most accurate predictions possible, while using the fewest resources possible. Scaling up an LLM requires increasing the model size, training dataset size, and compute (proportional to the first two’s product) in tandem following some <a href="https://arxiv.org/abs/2203.15556">optimal ratio</a>, limited by one of these three factors. For a chosen scaling policy, an LLM’s computational complexity in model capacity translates into efficiency using its training resources in general.</p> <p>With this mental model, we can understand the LLM Skeptic as making either the strong claim that LLMs memorize exclusively, or the weaker but more believable claim that they memorize excessively, leavened with only a little in-context learning. In short, LLMs are inefficient algorithms. The Skeptic would be wrong to pinpoint any given task as impossible in principle, if given unlimited parameters, training data, and compute. But they could be right that in practice inefficiency pushes generality out of reach. The LLM Scaler might variously reply that whatever way LLMs balance memorization and in-context learning is apparently good enough; that scaffolding will patch any inefficiencies; or that more efficient in-context learning strategies will keep emerging with scale.</p> <p>Will scaling up LLMs lead to AGI? And if not, what will? You can scale, but you can’t scale forever. The ultimate impact of current and future AI systems is bounded by the maximum efficiency with which they convert resources into solved problems. To understand their capabilities, we need to quantify this efficiency.</p>]]></content><author><name></name></author><category term="writings"/><category term="ai"/><category term="ai-safety"/><category term="computer-science"/><summary type="html"><![CDATA[To predict AI's impact, quantify its computational efficiency.]]></summary></entry><entry><title type="html">How Will AI Transform Civilization?</title><link href="https://aribrill.com/blog/2023/ai-civilization/" rel="alternate" type="text/html" title="How Will AI Transform Civilization?"/><published>2023-01-07T23:59:59+00:00</published><updated>2023-01-07T23:59:59+00:00</updated><id>https://aribrill.com/blog/2023/ai-civilization</id><content type="html" xml:base="https://aribrill.com/blog/2023/ai-civilization/"><![CDATA[<p>Something must be done, and a computer should do it for us. If it’s a simple task, like tracking a database of office supplies, we can write a software program that codes explicit rules telling the computer what to do:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">num_staplers</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">num_paperclips</span> <span class="o">=</span> <span class="n">num_staplers</span> <span class="o">+</span> <span class="mi">4</span>
<span class="k">while</span> <span class="n">num_paperclips</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
	<span class="n">num_paperclips</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure> <p>But what if the task is complex, like identifying dog breeds, writing a poem, or winning a game of chess? In these cases, we might not know good rules to code. Instead, we can teach the computer to learn the rules itself. This is the paradigm of machine learning, the bedrock of modern artificial intelligence (AI) systems like <a href="https://openai.com/blog/chatgpt/">ChatGPT</a>, <a href="https://stablediffusionweb.com/">Stable Diffusion</a>, and <a href="https://www.deepmind.com/blog/alphago-zero-starting-from-scratch">AlphaGo Zero</a>. In machine learning, an AI learns what to do by training on a dataset of examples. The AI model itself is a giant mathematical function, containing perhaps billions of parameters that must be learned. For this to work, we must provide three things: a training method, a training dataset, and lots of computing power.</p> <p>There are three basic kinds of training methods in machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, we give the AI examples of an input and a label saying the desired output:</p> <p>Input:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/norfolk_terrier-480.webp 480w,/assets/img/norfolk_terrier-800.webp 800w,/assets/img/norfolk_terrier-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/norfolk_terrier.png" class="img-fluid rounded z-depth-1" width="200" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Label: “Norfolk Terrier”</p> <p>Input:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/norwich_terrier-480.webp 480w,/assets/img/norwich_terrier-800.webp 800w,/assets/img/norwich_terrier-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/norwich_terrier.png" class="img-fluid rounded z-depth-1" width="200" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Label: “Norwich Terrier”</p> <p>(Image source: <a href="https://www.image-net.org/">ImageNet</a>)</p> <p>Supervised learning requires a well-defined problem, to list the possible labels, and known right answers, to match a label to each input. But what if we want the AI to write poetry – what are the “right answers” now? Instead, we can use unsupervised learning, a broad category of methods that don’t need external labels. One especially powerful unsupervised technique is self-supervised learning (SSL), which is how large language models like ChatGPT are trained. In SSL, labels come from the input data itself. For example, portions of the input might be masked and the AI trained to fill in the blank:</p> <p>Input: “Rough winds do shake the ____ buds of May”</p> <p>Label: “darling”</p> <p>SSL requires the AI to learn the underlying patterns in the training data, causing it to encode knowledge that can range from spelling and grammar to historical allusions and abstract reasoning. By virtue of its training task, the AI faithfully replicates the structure of the input data. It doesn’t judge like humans do, and will happily generate mediocrity as well as genius. A state-of-the-art strategy to get high-quality output from an SSL-trained AI is peppering its instructions with incantations like “top-rated”, “award-winning”, “hyper-realistic”, and “trending on ArtStation”, hoping it gives you something impressive.</p> <p>AI trained using SSL predicts what <em>is</em>, with no values or goals to tell it what <em>ought to be</em>. To create AI with goals, we can use reinforcement learning. In reinforcement learning, we provide the AI feedback on its actions instead of specifying labels. I certainly don’t know the best chess moves, and even Magnus Carlsen errs, but giving feedback to a chess-playing AI is easy: win the game!</p> <p>Reinforcement learning is generally data-hungry and feedback-sparse: an entire chess game boils down to one bit of information. As reward for its difficulty, it can create AI systems that exceed the bounds of human guidance and achievement. Reinforcement learning can also be used to train AI to emulate human values. ChatGPT uses a method called Reinforcement Learning from Human Feedback (<a href="https://arxiv.org/abs/1706.03741">RLHF</a>), learning from human ratings of its responses to provide answers that appear more helpful, harmless, and honest.</p> <p>Whether with supervised, unsupervised, or reinforcement learning, the art of machine learning is to create a system that generalizes the training data. It shouldn’t perfectly memorize what’s been seen, but instead infer simple underlying patterns that predict the never-before-seen. To do this, of course, there really must exist some simple underlying patterns. An ultimate AI given unlimited data and processor power still couldn’t predict <a href="https://www.amazon.com/Million-Random-Digits-Normal-Deviates/dp/0833030477">random numbers</a> better than chance.</p> <p>Multiple patterns might explain the same data. Consider the simplest situation in machine learning, the <em>C. Elegans</em> of AI: fitting a line to some data points. Suppose that a completely hypothetical novel disease has struck Hypothestan, and the number of cases is rising:</p> <center> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hypothetical_cases-480.webp 480w,/assets/img/hypothetical_cases-800.webp 800w,/assets/img/hypothetical_cases-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hypothetical_cases.jpg" class="img-fluid rounded z-depth-1" width="500" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>Will case numbers continue to climb at the same rate? Explode exponentially? Level off and start to decrease? Multiple predictions could be fully compatible with the data seen so far. Past experience, parsimony, and expert knowledge might give guidance, but in the end the best model is the one that best predicts the unseen data – what happens next. Nature is the final test.</p> <p>Increasingly powerful AI systems trained using machine learning will transform civilization, but the kinds of transformations they will bring about aren’t all the same. Civilization provides us benefits: technology and medicine; art and philosophy; natural resources; scientific, economic, social, and political systems. AI can level up an existing benefit, doing what’s already done cheaper, faster, fairer, better: this is a <em>sustaining innovation for civilization</em>. Or AI can create an entirely new benefit, something new under the sun: this is a <em>disruptive innovation for civilization</em>. What occurs depends on the training data. Human-generated training data lead to sustaining civilizational innovations, while data sourced from the external world enable disruptive ones. Sustaining civilizational innovations will upend economies; disruptive civilizational innovations will reshape the world.</p> <p>Human civilization provides many benefits, but those benefits aren’t evenly distributed. What I call a sustaining innovation for civilization (to adapt <a href="https://en.wikipedia.org/wiki/Disruptive_innovation">Clay Christiansen’s terms</a>) improves the world in essence by making available to everyone what the elite already have, and enhancing the lives of the elite at most a little.</p> <p>Imagine yourself scratching out a precarious sustenance as a medieval peasant farmer circa 1500. One day, a genie appears and magically transforms you into a landed noble. Hundreds of serfs now grow your crops, while dozens of servants prepare your food, weave your clothes, heat your home. You can live your life free from survival’s pressing needs.</p> <p>Suppose the genie now turns you into a modern-day middle-class American. In terms of daily necessities, you can get everything you had before and more from the local mall and grocery store. But now the nearest city is a half hour’s drive, not a day’s journey; another continent is a day’s flight, not six months’ sail. You can freely criticize both king and pope; learn about genes, governments, and galaxies; survive smallpox; eat coffee, chocolate, french fries, and sushi. You have access to books, ballots, and birth control.</p> <p>The average American’s daily energy consumption is <a href="https://www.withouthotair.com/">roughly equivalent</a> to the work done by 250 or so servants and laborers, powered primarily by fossil fuels and renewable electricity rather than by human drudgery. Sustaining civilizational innovations eliminated the peasant’s toil; disruptive civilizational innovations delivered everything else.</p> <p>AI systems trained on human-generated datasets enable sustaining civilizational innovations. Whether the training method is supervised learning using human-determined labels, unsupervised learning on human creations like writing and art, or reinforcement learning on human judgments, scaling up an AI model via clever architectures and massive server farms will allow it to replicate and generalize human behavior ever more faithfully. A system given only human input lives inside a human reference frame.</p> <p>Could such an AI ever jump out of the system and transcend its training? Many models can explain the same data, and it’s certainly possible for an AI to detect patterns different from what humans assume it would. It can lay bare humanity’s embarrassments, like a language model trained on tweets that spews cringeworthy hot takes. At worst, an AI could develop dangerously misaligned behaviors. For example, tuning an AI with RLHF to make it tell the “truth” might just teach it to flatter us, since who else is to judge? Or a model might seem to seriously miss the point, like an AI diagnostician of heart disease that ignores the patient’s electrocardiogram but cares a lot about the hospital’s address. At best, these divergences with expectations could spur our creativity, helping us think differently: why might two neighborhoods have disparate heart disease rates? These situations don’t present significant leaps over human capabilities, but instead reflect the gap between what humans believe of ourselves and what we truly are. Crucially, to see this gap we must invoke the real world’s objective truth, unfiltered to the extent possible by human subjectivity.</p> <p>Suppose an AI trained on human-generated input stumbled upon an internal representation of the world more powerful than the flawed human supervision it was given – not merely different, but in some way more complete, self-consistent, or true. For example, a dog, a chicken, a crow, and a fish are obviously all animals. That’s basic biology. But a peasant prioritizing survival over taxonomy might think in totally different categories – after all, <a href="https://astralcodexten.substack.com/p/somewhat-contra-marcus-on-ai-scaling">a person can eat a fish but not a crow</a>! Only a fool would group fish and crows together. Let’s pretend this peasant trains a creature-classifying AI to see the world his way. Now suppose that in the course of its training the AI hits upon the modern scientific concept of “animal”, which we can assume for argument’s sake really is the more elegant or broadly useful theory. What will happen as the AI is trained?</p> <p>If two theories make the same predictions, the simpler theory is <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">better</a>. Let’s say the scientific concept of an animal is simpler, and the AI uses it. However, since the peasant’s preferences dictate the “right” answers on the training data, the AI has to tack on a special exception every time there’s a discrepancy between the scientific and the folk concept. Eventually, all the exceptions will so lard up the scientific theory’s original elegance that the AI will be forced to discard it. Alternatively, the scientific theory might be better because it gives more accurate predictions than the folk theory. But again, the peasant is always “right”, so this can never be the case for data seen in training. Since cognitive resources are finite, training will optimize away a latent capability that has always been useless. Either way, an AI trained using human-generated datasets will tend to replicate human concepts and capabilities.</p> <p>Generative AI models like ChatGPT, <a href="https://openai.com/dall-e-2/">DALL-E</a>, and Stable Diffusion are largely sustaining for civilization. While the specific text and images they generate are new to humanity, the ability to generate them is not. Human authors and artists create comparable content. These models are civilizationally sustaining because their training data are human-made. It’s true their inputs may refer to things in the world: text mentions facts like the quadratic formula and George Washington’s birthday, and images include photos of garter snakes and mixing bowls. But these references are mediated entirely through human representations, unrooted in objective experience. The concepts aren’t connected to a predictive model of the external world. This could change in the future if language or image models are embedded within larger AI systems that do interact directly with the real world. For now, however, even purely sustaining AI systems can have massive economic and societal impact.</p> <p>First, AI is software. The upfront cost to develop and train an AI model may be high, but the marginal cost to run it is very low – perhaps not zero, but close enough compared to the cost of human labor so as to represent a qualitative shift enabling new business models and institutions. Automation can make processes faster, highly scalable, and less prone to mistakes, biases, and arbitrary delays.</p> <p>AI harnesses collective wisdom. In many cases, an AI model trained on a massive, diverse dataset can exceed the performance of any individual human. If your doctor diagnoses you with cancer, you might seek a second opinion; the advice of a panel of expert radiologists would be better yet; a hypothetical AI trained with knowledge of every cancer screening ever done might be best of all. AI’s improvement here comes from canceling out the idiosyncratic biases and gaps in experience we all have as individuals, equaling what humanity could accomplish if we could put our heads together.</p> <p>AI replaces drudgery. AI will automate boring, non-creative mental work just as machines have already automated routine physical work. Repetitive tasks like searching for information, filling out forms, and writing reports could disappear. While some jobs and chores may go away, I believe humanity has unlimited ingenuity and will always create new pointless busywork.</p> <p>Finally, AI democratizes expertise. Everyone will have easy access to custom-made art and essays written on demand. AI will make specialized knowledge from law, medicine, engineering, and science widely available, enhancing productivity and enriching our intellectual lives.</p> <p>A disruptive civilizational innovation, on the other hand, adds something entirely new to civilization. It provides what would be for everyone alive an unworkable technology, virgin resource, intractable algorithm, or unthinkable idea. These innovations have historically come from rediscovered ancient knowledge, trade with foreign civilizations, exploration and accident, and systematic scientific and intellectual progress. In today’s global world, only the last matters much, and that’s where AI matters too. Disruptive innovations for civilization affect everyone, rich or poor. Their long-term effects are difficult to predict.</p> <p>To create a disruptive civilizational innovation, an AI must be trained on data deriving directly from nature or generated by its own self-guided exploration, with minimal human processing. Whether using supervised, unsupervised, or reinforcement learning, human input cannot be the key ingredient, though it may still play important roles such as jump-starting training or teaching the AI human preferences and values. The kinds of AI systems that enable disruptive innovations for civilization include game-playing AIs trained using reinforcement learning like AlphaGo Zero, and AI systems for scientific and mathematical discovery like <a href="https://en.wikipedia.org/wiki/AlphaFold">AlphaFold</a> and <a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">AlphaTensor</a>. These models have made novel discoveries and achieved superhuman performance in their respective domains. Going forward, self-supervised learning may be a <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">promising paradigm</a> for building AI models with rich knowledge of particular domains and the world as a whole.</p> <p>How might AI’s disruptive civilizational impacts come about? As a starting point, all of the advantages of sustaining innovations for civilization that we’ve already discussed still hold. The potential impact has a high floor.</p> <p>Importantly, AI that doesn’t rely on human input for training can achieve qualitatively superhuman performance, accomplishing intellectual feats that exceed humankind’s collective wisdom. Even extraordinary humans can sometimes outwit all humanity. In 1999, reigning world chess champion Garry Kasparov played a team of 50,000 people coordinating over the internet in a game known as <a href="https://en.wikipedia.org/wiki/Kasparov_versus_the_World">Kasparov versus the World</a>. Kasparov won. Like chess, literature, and mathematics, some domains are genius-limited.</p> <p>We could simply treat a superhuman AI’s output as an accomplished feat that lays a foundation on which further research, development, and entrepreneurship can build. Einstein may have discovered relativity, but lesser lights than Einstein can still make use of his results. I can’t build a laptop from scratch, but I used one to write this essay. A protein structure predicted by AlphaFold can guide a biologist’s research just as would a published X-ray crystallography experiment.</p> <p>Alternatively, we could seek to learn from what a superhuman AI does, both to guide further exploration and research and to increase our understanding of the world for its own sake. This can first of all be done by examining the AI’s output, learning from the master’s example. For example, professional Go players have developed <a href="https://www.quora.com/What-are-some-novel-Go-strategies-discovered-by-AlphaGo-Zero/answer/Michael-Fu-1">new strategies</a> by studying games played by AlphaGo Zero. Furthermore, an AI is a giant mathematical function, and we can read out the calculations that form its thought processes. By developing methods for <a href="https://distill.pub/2020/circuits/zoom-in/">mechanistic interpretability</a> that let us reverse engineer these calculations into human-understandable algorithms, we could soon study not only what the AI does, but why it does it. An AI model, if it’s any good, encodes latent patterns in its training data that form the basis of its predictions. A method to interpret how an AI trained on scientific observations understands its training data could surface new principles of nature and expand the possibilities of <a href="https://distill.pub/2017/aia/">human thought</a>.</p> <p>Sustaining and disruptive civilizational innovations are distinguished in kind, not necessarily in absolute importance. While I believe that the disruptive innovations on the whole will have by far the greater long-term impact on humanity, this need not be true of any one given technology. Game-playing is a proof of concept of superhuman AI, not something of economic importance in itself. In the long term, sustaining civilizational innovations might have disruptive second-order effects if they free more people to pursue art, invention, and science.</p> <p>A disruptive civilizational innovation may have limited impact if it pertains to a domain where humans are already close to the upper bound on performance. There’s not a big real-world difference between the theoretically optimal self-driving car that can drive at maximum speed without ever having an accident, and a human professional driver who can merely drive extremely fast with extremely few accidents. This technology’s main benefit would come from its sustaining aspect – no need to pay a driver.</p> <p>A sustaining civilizational innovation can lead to a disruptive innovation in a particular business or market in the standard Christensen sense, and vice-versa. A robot consultant providing passable business advice for cents on McKinsey’s dollar is a disruptive innovation for a company but a sustaining one for civilization as a whole. Conversely, technologies based on new scientific discoveries can equally enrich existing megacorps as they can seed startups.</p> <p>AI’s apotheosis is artificial general intelligence (AGI). Every thinker defines AGI differently, but loosely, it would be an AI that does everything that an intelligent being can. A narrow AI system like those that exist today might summarize legal briefs or it might flip pancakes, but an AGI could do it all. With the concepts of sustaining and disruptive civilizational innovations we’ve developed in this essay, we can categorize AGI created using machine learning into two types. If an AGI were trained only on human-generated data, it would produce sustaining civilizational innovations. The most basic such AGI would be human-level: an AI that can do everything a normal human can do. It would pass the Turing Test. The ultimate such AGI would be humanity-level: an AI that can do everything any existing human or team of humans can do. Recent experience with large language models suggests that as models get bigger, performance improves and new capabilities emerge at a predictable rate. The <a href="https://www.gwern.net/Scaling-hypothesis">scaling hypothesis</a> holds that this trend will continue indefinitely. If true, the only ingredient needed to turn a human-level AGI humanity-level is scale. On the other hand, an AGI trained on self-generated or natural data could produce disruptive civilizational innovations. Such an AGI would be superhuman, capable of doing things no human can do. Holden Karnofsky’s concept of a Process for Automating Scientific and Technological Advancement or <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/">PASTA</a> would be a superhuman AGI in this sense.</p> <p>Rather than considering an AI’s capabilities, we can discuss its impact. Open Philanthropy defines <a href="https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit">Transformative AI</a> informally as “software that has at least as profound an impact on the world’s trajectory as the Industrial Revolution did”, in essence, AI that transforms civilization. Whether narrow or general, AI that transforms civilization will do so by generating either sustaining or disruptive civilizational innovations, or both. To predict what an AI might do, look to its training data.</p> <p>How will AI transform civilization? It’s possible science is essentially solved, and researchers just need to finalize a few details. If so, sustaining civilizational innovations from AI are most important. But if not – if today’s scientific knowledge is like a flickering candle next to nature’s shining sun – then disruptive civilizational innovations matter most.</p>]]></content><author><name></name></author><category term="writings"/><category term="ai"/><category term="science"/><category term="progress"/><category term="ai-safety"/><summary type="html"><![CDATA[Sustaining civilizational innovations will upend economies; disruptive civilizational innovations will reshape the world.]]></summary></entry></feed>